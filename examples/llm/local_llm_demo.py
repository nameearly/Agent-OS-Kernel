"""
Local LLM Demo - æœ¬åœ°æ¨¡å‹æ¼”ç¤º

æ¼”ç¤ºå¦‚ä½•åœ¨ Agent OS Kernel ä¸­ä½¿ç”¨æœ¬åœ°æ¨¡å‹
"""

import asyncio
import sys
sys.path.insert(0, '.')


async def demo_ollama():
    """Ollama æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("Demo: Ollama Local Model")
    print("=" * 60)
    
    from agent_os_kernel.llm import LLMProviderFactory, LLMConfig
    
    try:
        factory = LLMProviderFactory()
        
        # Ollama é…ç½®
        provider = factory.create(LLMConfig(
            provider="ollama",
            model="qwen2.5:7b",
            base_url="http://localhost:11434"
        ))
        
        print(f"âœ“ Ollama provider created: {provider.provider_name}")
        print(f"  Model: {provider.model}")
        
        # æµ‹è¯•èŠå¤©
        messages = [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
        ]
        
        result = await provider.chat(messages)
        print(f"âœ“ Response: {result.get('content', '')[:100]}...")
        
    except Exception as e:
        print(f"âš ï¸ Ollama not available: {e}")
        print("   Install: curl -fsSL https://ollama.ai | sh")
        print("   Then: ollama pull qwen2.5:7b")


async def demo_mock_local():
    """Mock æœ¬åœ°æ¨¡å‹æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("Demo: Mock Local Model (Development)")
    print("=" * 60)
    
    from agent_os_kernel.llm import create_mock_provider
    
    provider = create_mock_provider()
    print(f"âœ“ Mock provider created: {provider.provider_name}")
    
    # è®¾ç½®æœ¬åœ°é£æ ¼å“åº”
    provider.set_response("local", "æˆ‘æ˜¯ä¸€ä¸ªæœ¬åœ°è¿è¡Œçš„ AI åŠ©æ‰‹ï¼Œä½¿ç”¨ Ollama/vLLM æä¾›æ”¯æŒã€‚")
    
    messages = [{"role": "user", "content": "local"}]
    result = await provider.chat(messages)
    print(f"âœ“ Response: {result.get('content', '')}")


async def demo_factory():
    """å·¥å‚æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("Demo: Provider Factory")
    print("=" * 60)
    
    from agent_os_kernel.llm import LLMProviderFactory
    
    factory = LLMProviderFactory()
    
    # åˆ—å‡ºæ‰€æœ‰ Provider
    providers = factory.list_providers()
    print(f"âœ“ Total providers: {len(providers)}")
    
    for info in providers:
        local = "ğŸ " if info.local else "â˜ï¸"
        print(f"  {local} {info.name}: {info.description}")
    
    # è¿‡æ»¤æœ¬åœ° Provider
    local_providers = [p for p in providers if p.local]
    print(f"\nâœ“ Local providers: {len(local_providers)}")
    for p in local_providers:
        print(f"  ğŸ  {p.name}: {p.default_model}")


async def demo_config():
    """é…ç½®æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("Demo: Configuration")
    print("=" * 60)
    
    import yaml
    
    # Ollama é…ç½®
    ollama_config = {
        "llm": {
            "providers": [
                {
                    "name": "ollama-local",
                    "provider": "ollama",
                    "model": "qwen2.5:7b",
                    "base_url": "http://localhost:11434",
                    "temperature": 0.7,
                    "max_tokens": 4096
                }
            ]
        }
    }
    
    print("ğŸ“„ Ollama Config:")
    print(yaml.dump(ollama_config, default_flow_style=False))
    
    # vLLM é…ç½®
    vllm_config = {
        "llm": {
            "providers": [
                {
                    "name": "vllm-gpu",
                    "provider": "vllm",
                    "model": "meta-llama/Llama-3.1-8B-Instruct",
                    "base_url": "http://localhost:8000/v1",
                    "temperature": 0.1,
                    "max_tokens": 8192
                }
            ]
        }
    }
    
    print("ğŸ“„ vLLM Config:")
    print(yaml.dump(vllm_config, default_flow_style=False))


async def demo_kernel_with_local():
    """å†…æ ¸é›†æˆæ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("Demo: Kernel with Local Model")
    print("=" * 60)
    
    from agent_os_kernel import AgentOSKernel
    
    kernel = AgentOSKernel()
    print("âœ“ Kernel initialized")
    
    # ä½¿ç”¨é»˜è®¤ Provider (å¯ä»¥é…ç½®ä¸º Ollama)
    kernel.print_status()
    
    # æ˜¾ç¤ºå¯ç”¨å·¥å…·
    registry = kernel.tool_registry
    stats = registry.get_stats()
    print(f"âœ“ Available tools: {stats['total_tools']}")


async def demo_comparison():
    """æ€§èƒ½å¯¹æ¯”æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("Demo: Local vs Cloud Comparison")
    print("=" * 60)
    
    from agent_os_kernel.llm import create_mock_provider
    
    print("\nğŸ“Š Cost Comparison:")
    print("-" * 40)
    print(f"{'Provider':<20} {'Cost/1M Tokens':<15} {'Privacy':<10}")
    print("-" * 40)
    print(f"{'Ollama (Local)':<20} {'$0.00':<15} {'âœ… Full':<10}")
    print(f"{'vLLM (Local)':<20} {'$0.00':<15} {'âœ… Full':<10}")
    print(f"{'OpenAI GPT-4':<20} {'$30.00':<15} {'âŒ Data sent':<10}")
    print(f"{'Claude 3.5':<20} {'$15.00':<15} {'âŒ Data sent':<10}")
    print(f"{'DeepSeek':<20} {'$0.28':<15} {'âŒ Data sent':<10}")
    
    print("\nğŸš€ Advantages of Local Models:")
    print("  âœ… éšç§ä¿æŠ¤ - æ•°æ®ä¸ç¦»å¼€æœ¬åœ°")
    print("  âœ… é›¶æˆæœ¬ - æ— éœ€ API è´¹ç”¨")
    print("  âœ… ç¦»çº¿å¯ç”¨ - æ— éœ€ç½‘ç»œ")
    print("  âœ… å®šåˆ¶åŒ– - å¯å¾®è°ƒæ¨¡å‹")
    print("  âœ… é€Ÿåº¦å¿« - æ— ç½‘ç»œå»¶è¿Ÿ")
    
    print("\nâš ï¸ Considerations:")
    print("  âš ï¸ éœ€è¦ GPU æ˜¾å­˜")
    print("  âš ï¸ æ¨¡å‹å¤§å°é™åˆ¶")
    print("  âš ï¸ ç»´æŠ¤æˆæœ¬")


async def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("=" * 60)
    print("ğŸš€ Agent OS Kernel - Local LLM Demo")
    print("=" * 60)
    
    await demo_factory()
    await demo_mock_local()
    await demo_config()
    await demo_kernel_with_local()
    await demo_comparison()
    
    print("\n" + "=" * 60)
    print("âœ… Local LLM Demo Complete!")
    print("=" * 60)
    print("\nğŸ“š Learn More:")
    print("   docs/local-models.md")
    print("   https://ollama.ai")
    print("   https://docs.vllm.ai")


if __name__ == "__main__":
    asyncio.run(main())
