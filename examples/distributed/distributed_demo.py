"""
åˆ†å¸ƒå¼å’Œç›‘æ§ç¤ºä¾‹

å±•ç¤ºï¼š
1. åˆ†å¸ƒå¼è°ƒåº¦å™¨
2. Agent çƒ­è¿ç§»
3. GPU èµ„æºç›‘æ§
4. ç³»ç»Ÿç›‘æ§å’Œå‘Šè­¦
"""

import asyncio
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agent_os_kernel.distributed import (
    DistributedScheduler,
    AgentMigration,
    create_distributed_scheduler
)
from agent_os_kernel.resources import (
    GPUMonitor,
    GPUManager,
    SystemMonitor,
    MetricsCollector
)


async def demo_distributed_scheduler():
    """åˆ†å¸ƒå¼è°ƒåº¦å™¨ç¤ºä¾‹"""
    print("=" * 60)
    print("åˆ†å¸ƒå¼è°ƒåº¦å™¨ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = create_distributed_scheduler(
        node_id="scheduler-1",
        host="localhost",
        port=8001
    )
    
    # æ³¨å†ŒèŠ‚ç‚¹
    await scheduler.register_node(
        node_id="node-1",
        hostname="gpu-server-1",
        port=8002,
        capabilities=["gpu", "nvidia"],
        resources={"gpu_memory": 16000, "cost_per_hour": 2.0}
    )
    
    await scheduler.register_node(
        node_id="node-2",
        hostname="cpu-server-1",
        port=8003,
        capabilities=["cpu", "high_memory"],
        resources={"memory_gb": 64, "cost_per_hour": 0.5}
    )
    
    await scheduler.register_node(
        node_id="node-3",
        hostname="gpu-server-2",
        port=8004,
        capabilities=["gpu", "nvidia"],
        resources={"gpu_memory": 32000, "cost_per_hour": 3.0}
    )
    
    print("\nğŸ“Š èŠ‚ç‚¹æ³¨å†Œå®Œæˆ")
    
    # å¯åŠ¨è°ƒåº¦å™¨
    await scheduler.start()
    
    # æäº¤ä»»åŠ¡
    tasks = [
        ("task-1", {"name": "GPU_Agent_1"}, "gpu-server"),
        ("task-2", {"name": "CPU_Agent_1"}, "cpu-server"),
        ("task-3", {"name": "GPU_Agent_2"}, "gpu-server"),
    ]
    
    for task_id, config, _ in tasks:
        await scheduler.submit_task(
            task_id=task_id,
            agent_config=config,
            priority=1
        )
    
    # ç­‰å¾…è°ƒåº¦
    await asyncio.sleep(1)
    
    # è·å–é›†ç¾¤çŠ¶æ€
    status = await scheduler.get_cluster_status()
    
    print(f"\nğŸ“ˆ é›†ç¾¤çŠ¶æ€:")
    print(f"   èŠ‚ç‚¹æ•°: {status['total_nodes']}")
    print(f"   æ€» Agents: {status['total_agents']}")
    print(f"   å¹³å‡è´Ÿè½½: {status['avg_load']:.2f}")
    print(f"   å¾…å¤„ç†ä»»åŠ¡: {status['pending_tasks']}")
    
    for node in status["nodes"]:
        print(f"   - {node['node_id']}: {node['agents_count']} agents, load={node['load']:.2f}")
    
    await scheduler.stop()
    
    return scheduler


async def demo_agent_migration():
    """Agent è¿ç§»ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("Agent çƒ­è¿ç§»ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºè¿ç§»ç®¡ç†å™¨
    migration = AgentMigration(storage_dir="./demo_migrations")
    
    # æ¨¡æ‹Ÿ Agent çŠ¶æ€
    agent_state = {
        "agent_id": "agent-demo-1",
        "name": "DemoAgent",
        "status": "running",
        "progress": 0.75
    }
    
    context = [
        {"role": "user", "content": "ä»»åŠ¡ 1"},
        {"role": "assistant", "content": "å›å¤ 1"},
        {"role": "user", "content": "ä»»åŠ¡ 2"},
    ]
    
    memory = {
        "learned_facts": ["fact1", "fact2"],
        "preferences": {"theme": "dark"}
    }
    
    tools_state = {
        "tool1": {"usage_count": 5},
        "tool2": {"usage_count": 3}
    }
    
    print("\nğŸ“ åˆ›å»ºæ£€æŸ¥ç‚¹...")
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹
    checkpoint_id = await migration.create_checkpoint(
        agent_id="agent-demo-1",
        state=agent_state,
        context=context,
        memory=memory,
        tools_state=tools_state
    )
    
    print(f"   æ£€æŸ¥ç‚¹: {checkpoint_id}")
    
    # æ¨¡æ‹Ÿè¿ç§»
    print("\nğŸš€ æ‰§è¡Œè¿ç§»...")
    result = await migration.migrate(
        agent_id="agent-demo-1",
        source_node="node-1",
        target_node="node-2",
        checkpoint_id=checkpoint_id
    )
    
    print(f"   è¿ç§»ç»“æœ:")
    print(f"   - æˆåŠŸ: {result['success']}")
    print(f"   - æºèŠ‚ç‚¹: {result['source_node']}")
    print(f"   - ç›®æ ‡èŠ‚ç‚¹: {result['target_node']}")
    print(f"   - è€—æ—¶: {result['duration_seconds']}s")
    
    # ä»æ£€æŸ¥ç‚¹æ¢å¤
    print("\nğŸ“– ä»æ£€æŸ¥ç‚¹æ¢å¤...")
    restored = await migration.restore_from_checkpoint(checkpoint_id)
    
    if restored:
        print(f"   âœ… æ¢å¤æˆåŠŸ")
        print(f"   - Agent çŠ¶æ€: {restored['agent_state']['name']}")
        print(f"   - ä¸Šä¸‹æ–‡æ•°: {len(restored['context'])}")
        print(f"   - è®°å¿†é¡¹: {len(restored['memory'])}")
    
    # åˆ—å‡ºæ£€æŸ¥ç‚¹
    checkpoints = await migration.list_checkpoints()
    print(f"\nğŸ“‹ æ£€æŸ¥ç‚¹åˆ—è¡¨: {len(checkpoints)} ä¸ª")
    
    return migration


async def demo_gpu_monitor():
    """GPU ç›‘æ§ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("GPU èµ„æºç›‘æ§ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»º GPU ç®¡ç†å™¨
    manager = GPUManager()
    
    print("\nğŸ” æ£€æµ‹ GPU è®¾å¤‡...")
    
    # æ£€æµ‹è®¾å¤‡
    devices = await manager.monitor.detect_devices()
    
    if devices:
        print(f"   å‘ç° {len(devices)} ä¸ª GPU:")
        
        for device in devices:
            print(f"   - GPU {device.index}: {device.name}")
            print(f"     æ˜¾å­˜: {device.memory_used_mb}/{device.memory_total_mb} MB")
            print(f"     åˆ©ç”¨ç‡: {device.utilization_percent}%")
            print(f"     æ¸©åº¦: {device.temperature_c}Â°C")
        
        # è·å–çŠ¶æ€
        status = await manager.get_status()
        print(f"\nğŸ“Š GPU çŠ¶æ€:")
        print(f"   è®¾å¤‡æ•°: {status['devices_count']}")
        print(f"   åˆå§‹åŒ–: {status['initialized']}")
    else:
        print("   âš ï¸ æœªæ£€æµ‹åˆ° GPU è®¾å¤‡ (å¯èƒ½æ²¡æœ‰ NVIDIA GPU)")
    
    return manager


async def demo_system_monitor():
    """ç³»ç»Ÿç›‘æ§ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç³»ç»Ÿç›‘æ§ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = SystemMonitor()
    
    # æ³¨å†Œå‘Šè­¦å›è°ƒ
    async def handle_alert(alert):
        print(f"   ğŸ”” å‘Šè­¦: {alert.title} - {alert.description}")
    
    monitor.on_alert(handle_alert)
    
    print("\nğŸ“Š é‡‡é›†ç³»ç»ŸæŒ‡æ ‡...")
    
    # é‡‡é›†æŒ‡æ ‡
    metrics = await monitor.collect_metrics()
    
    print(f"   CPU: {metrics['cpu_percent']:.1f}%")
    print(f"   å†…å­˜: {metrics['memory_percent']:.1f}% ({metrics['memory_used_mb']:.0f} MB)")
    print(f"   ç£ç›˜: {metrics['disk_usage_percent']:.1f}%")
    print(f"   è¿›ç¨‹æ•°: {metrics['process_count']}")
    
    # è·å–æ‘˜è¦
    summary = monitor.get_summary()
    
    print(f"\nğŸ“ˆ ç›‘æ§æ‘˜è¦:")
    print(f"   ç›‘æ§çŠ¶æ€: {'è¿è¡Œä¸­' if summary['monitoring'] else 'æœªè¿è¡Œ'}")
    print(f"   æ´»è·ƒå‘Šè­¦: {summary['active_alerts']}")
    print(f"   è§„åˆ™æ•°: {summary['rules_count']}")
    
    # æŒ‡æ ‡æ”¶é›†å™¨
    collector = MetricsCollector()
    
    print(f"\nğŸ“‰ æŒ‡æ ‡æ”¶é›†:")
    collector.counter_inc("requests_total", 10)
    collector.counter_inc("requests_total", 5)
    collector.gauge_set("active_agents", 5)
    collector.histogramObserve("response_time_ms", 150)
    collector.histogramObserve("response_time_ms", 200)
    collector.histogramObserve("response_time_ms", 100)
    
    stats = collector.get_all()
    
    print(f"   è¯·æ±‚æ•°: {stats['counters']['requests_total']}")
    print(f"   æ´»è·ƒ Agents: {stats['gauges']['active_agents']}")
    hist_stats = stats['histograms']['response_time_ms']
    print(f"   å“åº”æ—¶é—´:")
    print(f"     - å¹³å‡: {hist_stats.get('avg', 0):.1f}ms")
    print(f"     - P95: {hist_stats.get('p95', 0):.1f}ms")
    
    return monitor


async def demo_complete_pipeline():
    """å®Œæ•´æµæ°´çº¿ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("å®Œæ•´ç›‘æ§æµæ°´çº¿")
    print("=" * 60)
    
    # åˆ›å»ºç»„ä»¶
    scheduler = create_distributed_scheduler("main-scheduler")
    migration = AgentMigration()
    gpu_manager = GPUManager()
    monitor = SystemMonitor()
    collector = MetricsCollector()
    
    # æ³¨å†Œå‘Šè­¦å¤„ç†
    async def on_alert(alert):
        print(f"   ğŸš¨ [{alert.level.value.upper()}] {alert.title}")
    
    monitor.on_alert(on_alert)
    
    # å¯åŠ¨ç›‘æ§
    await monitor.start_monitoring()
    
    # ç­‰å¾…æ”¶é›†ä¸€äº›æ•°æ®
    await asyncio.sleep(2)
    
    # æ¨¡æ‹Ÿè´Ÿè½½
    collector.counter_inc("tasks_submitted", 10)
    collector.counter_inc("tasks_completed", 8)
    collector.gauge_set("cluster_nodes", 3)
    collector.histogramObserve("task_duration_s", 1.5)
    collector.histogramObserve("task_duration_s", 2.0)
    
    # åœæ­¢ç›‘æ§
    await monitor.stop_monitoring()
    
    # å¯¼å‡ºæŒ‡æ ‡
    export = monitor.export_metrics(300)
    
    print(f"\nğŸ“Š æŒ‡æ ‡å¯¼å‡º:")
    print(f"   æŒ‡æ ‡ç±»å‹: {len(export['metrics'])}")
    print(f"   æŒç»­æ—¶é—´: {export['duration_seconds']}s")
    
    # è·å–å‘Šè­¦
    alerts = monitor.get_alerts(unresolved=True)
    print(f"   æœªè§£å†³å‘Šè­¦: {len(alerts)}")
    
    return {
        "scheduler": scheduler,
        "migration": migration,
        "gpu_manager": gpu_manager,
        "monitor": monitor,
        "collector": collector
    }


async def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ åˆ†å¸ƒå¼å’Œç›‘æ§ç¤ºä¾‹")
    print("=" * 60)
    
    # 1. åˆ†å¸ƒå¼è°ƒåº¦å™¨
    await demo_distributed_scheduler()
    
    # 2. Agent è¿ç§»
    await demo_agent_migration()
    
    # 3. GPU ç›‘æ§
    await demo_gpu_monitor()
    
    # 4. ç³»ç»Ÿç›‘æ§
    await demo_system_monitor()
    
    # 5. å®Œæ•´æµæ°´çº¿
    await demo_complete_pipeline()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
